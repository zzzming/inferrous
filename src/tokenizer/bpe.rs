//! Clean tokenizer implementation for inferrous
//! 
//! Shows both professional and educational approaches to tokenization:
//! 1. Production-ready using `tokenizers` crate (like Candle does)
//! 2. Educational from-scratch BPE implementation
//!
//! **Important**: Tokenizers don't need tensor operations!
//! They work with simple data structures: HashMap, Vec, String

use anyhow::Result;
use std::collections::HashMap;
use std::path::Path;
use super::{TokenizerConfig, SpecialTokens, TokenizerBuilder, TokenizerType, ProductionTokenizer, EncodingResult, Tokenizer, PaddingStrategy, ChatMessage};

/// Educational from-scratch tokenizer to understand how BPE works
/// This shows the internal mechanics without heavy dependencies
pub struct BPETokenizer {
    /// Core vocabulary: token string -> token ID
    vocab: HashMap<String, u32>,
    /// Reverse mapping: token ID -> token string
    id_to_token: HashMap<u32, String>,
    /// BPE merge rules: (token1, token2) -> merged_token
    merges: HashMap<(String, String), String>,
    /// Special token IDs
    special_tokens: SpecialTokens,
}

impl BPETokenizer {
    /// Create a basic tokenizer with byte-level vocabulary
    pub fn new() -> Self {
        let mut vocab = HashMap::new();
        let mut id_to_token = HashMap::new();
        
        // Add special tokens first
        let special_tokens = SpecialTokens {
            unk: 0,
            bos: 1, 
            eos: 2,
            pad: 3,
        };
        
        let special_pairs = [
            (special_tokens.unk, "<unk>"),
            (special_tokens.bos, "<s>"),
            (special_tokens.eos, "</s>"),
            (special_tokens.pad, "<pad>"),
        ];
        
        for (id, token) in special_pairs {
            vocab.insert(token.to_string(), id);
            id_to_token.insert(id, token.to_string());
        }
        
        // Add byte-level tokens (256 possible bytes)
        for byte_val in 0..256 {
            let token = format!("<0x{:02X}>", byte_val);
            let id = byte_val as u32 + 4; // Start after special tokens
            vocab.insert(token.clone(), id);
            id_to_token.insert(id, token);
        }
        
        Self {
            vocab,
            id_to_token,
            merges: HashMap::new(),
            special_tokens,
        }
    }

    /// Load vocabulary and merges from a tokenizer.json file
    /// This shows how to parse the real format without external crates
    pub fn from_tokenizer_json(path: &str) -> Result<Self> {
        println!("ðŸ“š Loading educational tokenizer from: {}", path);
        
        let content = std::fs::read_to_string(path)?;
        let json: serde_json::Value = serde_json::from_str(&content)?;
        
        let mut tokenizer = Self::new();
        
        // Parse vocabulary from the JSON structure
        if let Some(model) = json.get("model") {
            if let Some(vocab) = model.get("vocab") {
                if let Some(vocab_obj) = vocab.as_object() {
                    println!("   ðŸ“– Loading {} vocabulary entries", vocab_obj.len());
                    
                    // Replace our basic vocab with the real one
                    tokenizer.vocab.clear();
                    tokenizer.id_to_token.clear();
                    
                    for (token, id_val) in vocab_obj {
                        if let Some(id) = id_val.as_u64() {
                            let id = id as u32;
                            tokenizer.vocab.insert(token.clone(), id);
                            tokenizer.id_to_token.insert(id, token.clone());
                        }
                    }
                }
            }
            
            // Parse BPE merge rules
            if let Some(merges) = model.get("merges") {
                if let Some(merges_array) = merges.as_array() {
                    println!("   ðŸ”— Loading {} BPE merge rules", merges_array.len());
                    
                    for merge in merges_array {
                        if let Some(merge_str) = merge.as_str() {
                            let parts: Vec<&str> = merge_str.split_whitespace().collect();
                            if parts.len() == 2 {
                                let merged = format!("{}{}", parts[0], parts[1]);
                                tokenizer.merges.insert(
                                    (parts[0].to_string(), parts[1].to_string()),
                                    merged
                                );
                            }
                        }
                    }
                }
            }
        }
        
        // Update special tokens from added_tokens section
        if let Some(added_tokens) = json.get("added_tokens") {
            if let Some(tokens_array) = added_tokens.as_array() {
                for token_obj in tokens_array {
                    if let (Some(id), Some(content)) = (token_obj.get("id"), token_obj.get("content")) {
                        if let (Some(id_num), Some(content_str)) = (id.as_u64(), content.as_str()) {
                            let id = id_num as u32;
                            
                            // Update special token mappings
                            match content_str {
                                "<s>" | "<|im_start|>" => tokenizer.special_tokens.bos = id,
                                "</s>" | "<|im_end|>" | "<|endoftext|>" => tokenizer.special_tokens.eos = id,
                                "<unk>" => tokenizer.special_tokens.unk = id,
                                "<pad>" => tokenizer.special_tokens.pad = id,
                                _ => {}
                            }
                        }
                    }
                }
            }
        }
        
        println!("âœ… Educational tokenizer loaded with {} tokens", tokenizer.vocab.len());
        Ok(tokenizer)
    }
     /// Convert text to initial byte-level tokens
     fn text_to_bytes(&self, text: &str) -> Vec<String> {
        text.bytes()
            .map(|byte| format!("<0x{:02X}>", byte))
            .collect()
    }

    /// Apply one round of BPE merges to the token sequence
    fn apply_bpe_merges(&self, tokens: &[String]) -> Vec<String> {
        if tokens.len() < 2 {
            return tokens.to_vec();
        }

        // Find the first pair that can be merged
        for i in 0..(tokens.len() - 1) {
            let pair = (tokens[i].clone(), tokens[i + 1].clone());
            if let Some(merged_token) = self.merges.get(&pair) {
                // Apply the merge
                let mut result = Vec::new();
                result.extend_from_slice(&tokens[..i]);
                result.push(merged_token.clone());
                result.extend_from_slice(&tokens[i + 2..]);
                return result;
            }
        }

        // No merges found
        tokens.to_vec()
    }
}

impl Tokenizer for BPETokenizer {
    /// Encode text to token IDs using BPE algorithm step by step
    fn encode(&self, text: &str) -> Result<Vec<u32>> {
        // Step 1: Convert to byte-level tokens
        let mut tokens = self.text_to_bytes(text);
        
        // Step 2: Apply BPE merges iteratively
        loop {
            let merged = self.apply_bpe_merges(&tokens);
            if merged == tokens {
                break; // No more merges possible
            }
            tokens = merged;
        }
        
        // Step 3: Convert tokens to IDs
        let token_ids: Vec<u32> = tokens
            .iter()
            .map(|token| self.vocab.get(token).copied().unwrap_or(self.special_tokens.unk))
            .collect();
        
        Ok(token_ids)
    }

    /// Decode token IDs back to text
    fn decode(&self, token_ids: &[u32]) -> Result<String> {
        let tokens: Vec<String> = token_ids
            .iter()
            .map(|&id| {
                self.id_to_token
                    .get(&id)
                    .cloned()
                    .unwrap_or_else(|| self.id_to_token[&self.special_tokens.unk].clone())
            })
            .collect();
        
        // Convert tokens back to text (simplified - real implementation is more complex)
        let mut text = String::new();
        for token in tokens {
            if token.starts_with("<0x") && token.ends_with(">") {
                // Byte token - convert back to character
                if let Ok(byte_val) = u8::from_str_radix(&token[3..token.len()-1], 16) {
                    text.push(byte_val as char);
                }
            } else if token.starts_with('<') && token.ends_with('>') {
                // Special tokens - include them in output for test visibility
                match token.as_str() {
                    "<unk>" | "<s>" | "</s>" | "<pad>" => {
                        // Skip these in normal text, but for unknown tokens, show something
                        if &token == &self.id_to_token[&self.special_tokens.unk] {
                            text.push_str("[UNK]");
                        }
                    }
                    _ => {
                        // Other special tokens, skip
                    }
                }
            } else {
                // Regular token - add directly
                text.push_str(&token);
            }
        }
        
        Ok(text)
    }

    /// Get vocabulary size
    fn vocab_size(&self) -> usize {
        self.vocab.len()
    }

    /// Get token ID for a specific token
    fn token_to_id(&self, token: &str) -> Option<u32> {
        self.vocab.get(token).copied()
    }

    /// Enhanced encoding with configuration
    fn encode_with_config(&self, text: &str, config: &TokenizerConfig) -> Result<EncodingResult> {
        let mut input_ids = self.encode(text)?;
        
        // Apply truncation if specified
        if let Some(max_len) = config.max_length {
            if config.truncation && input_ids.len() > max_len {
                input_ids.truncate(max_len);
            }
        }
        
        // Generate attention mask
        let attention_mask = vec![1u32; input_ids.len()];
        
        // TODO: Implement proper padding strategies (MaxLength, etc.)
        // TODO: Add real offset tracking during tokenization
        // TODO: Add token type IDs support
        
        Ok(EncodingResult {
            input_ids,
            attention_mask,
            token_type_ids: None,
            offsets: if config.return_offsets {
                // TODO: Track actual character positions during tokenization
                Some(vec![(0, text.len())])
            } else {
                None
            },
        })
    }

    /// Batch processing - encode multiple texts at once
    fn encode_batch(&self, texts: &[&str]) -> Result<Vec<Vec<u32>>> {
        texts.iter()
            .map(|text| self.encode(text))
            .collect()
    }

    /// Batch encoding with configuration
    fn encode_batch_with_config(&self, texts: &[&str], config: &TokenizerConfig) -> Result<Vec<EncodingResult>> {
        let mut results: Result<Vec<_>> = texts.iter()
            .map(|text| self.encode_with_config(text, config))
            .collect();
            
        let mut results = results?;
        
        // Apply batch-level padding if needed
        if config.padding == PaddingStrategy::LongestFirst && texts.len() > 1 {
            let max_len = results.iter().map(|r| r.input_ids.len()).max().unwrap_or(0);
            
            for result in &mut results {
                let current_len = result.input_ids.len();
                if current_len < max_len {
                    let pad_token_id = self.special_tokens.pad;
                    result.input_ids.extend(vec![pad_token_id; max_len - current_len]);
                    result.attention_mask.extend(vec![0; max_len - current_len]);
                }
            }
        }
        
        Ok(results)
    }

    /// Decode multiple token sequences
    fn decode_batch(&self, token_ids_batch: &[&[u32]]) -> Result<Vec<String>> {
        token_ids_batch.iter()
            .map(|token_ids| self.decode(token_ids))
            .collect()
    }

    /// Get special tokens dictionary
    fn get_special_tokens(&self) -> HashMap<String, u32> {
        let mut special_tokens = HashMap::new();
        special_tokens.insert("bos_token".to_string(), self.special_tokens.bos);
        special_tokens.insert("eos_token".to_string(), self.special_tokens.eos);
        special_tokens.insert("unk_token".to_string(), self.special_tokens.unk);
        special_tokens.insert("pad_token".to_string(), self.special_tokens.pad);
        special_tokens
    }

    /// Add special tokens to vocabulary
    fn add_special_tokens(&mut self, tokens: &HashMap<String, u32>) -> Result<()> {
        for (token, &id) in tokens {
            self.vocab.insert(token.clone(), id);
            self.id_to_token.insert(id, token.clone());
            
            // Update special token IDs if they match known patterns
            match token.as_str() {
                "<s>" | "<|im_start|>" => self.special_tokens.bos = id,
                "</s>" | "<|im_end|>" | "<|endoftext|>" => self.special_tokens.eos = id,
                "<unk>" => self.special_tokens.unk = id,
                "<pad>" => self.special_tokens.pad = id,
                _ => {}
            }
        }
        Ok(())
    }

    /// Text normalization
    fn normalize(&self, text: &str) -> String {
        // TODO: Implement proper text normalization
        // TODO: Add Unicode normalization (NFC, NFKC)
        // TODO: Add case handling, accent removal
        // TODO: Add custom normalization rules per model
        
        // Basic normalization for now
        text.trim().to_string()
    }

    /// Pre-tokenization - split text into tokens before BPE
    fn pre_tokenize(&self, text: &str) -> Vec<String> {
        // TODO: Implement proper pre-tokenization
        // TODO: Add whitespace splitting with proper handling
        // TODO: Add punctuation separation
        // TODO: Add language-specific rules
        // TODO: Handle contractions and special cases
        
        // Basic whitespace splitting for now
        text.split_whitespace()
            .map(|s| s.to_string())
            .collect()
    }

    /// Apply chat template for conversations
    fn apply_chat_template(&self, messages: &[ChatMessage]) -> Result<String> {
        // TODO: Implement proper chat template support
        // TODO: Support Llama-2 chat format: <s>[INST] {user} [/INST] {assistant} </s>
        // TODO: Support ChatML format: <|im_start|>role\ncontent<|im_end|>
        // TODO: Make template configurable per model
        
        // Simple implementation for now
        let mut formatted = String::new();
        
        for (i, msg) in messages.iter().enumerate() {
            match msg.role.as_str() {
                "system" => {
                    formatted.push_str(&format!("<<SYS>>\n{}\n<</SYS>>\n\n", msg.content));
                }
                "user" => {
                    if i == 0 || messages.get(i-1).map(|m| m.role.as_str()) != Some("system") {
                        formatted.push_str(&format!("[INST] {} [/INST] ", msg.content));
                    } else {
                        formatted.push_str(&format!("[INST] {} [/INST] ", msg.content));
                    }
                }
                "assistant" => {
                    formatted.push_str(&format!("{} ", msg.content));
                }
                _ => {
                    formatted.push_str(&format!("{}: {} ", msg.role, msg.content));
                }
            }
        }
        
        Ok(formatted.trim().to_string())
    }

    /// Get token string from ID
    fn get_id_to_token(&self, id: u32) -> Option<String> {
        self.id_to_token.get(&id).cloned()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::Path;

    const TEST_TOKENIZER_PATH: &str = "tests/test_tokenizer.json";

    /// Test the TokenizerBuilder pattern with BPE tokenizer
    #[test]
    fn test_tokenizer_builder_bpe() {
        // Skip if test tokenizer file doesn't exist
        if !Path::new(TEST_TOKENIZER_PATH).exists() {
            println!("âš ï¸  Test tokenizer file not found, skipping BPE test");
            return;
        }

        let builder = TokenizerBuilder::new(
            Some(TokenizerType::BPE), 
            TEST_TOKENIZER_PATH.to_string()
        );
        
        let tokenizer = builder.build().expect("Failed to build BPE tokenizer");
        
        // Test basic functionality through trait
        assert!(tokenizer.vocab_size() > 0);
        
        // Test encoding simple text
        let text = "hello world";
        let tokens = tokenizer.encode(text).expect("Failed to encode");
        assert!(!tokens.is_empty());
        
        // Test decoding
        let decoded = tokenizer.decode(&tokens).expect("Failed to decode");
        assert!(!decoded.is_empty());
        
        // Test special token lookup
        assert_eq!(tokenizer.token_to_id("<s>"), Some(1));
        assert_eq!(tokenizer.token_to_id("</s>"), Some(2));
        assert_eq!(tokenizer.token_to_id("<unk>"), Some(0));
    }

    /// Test the TokenizerBuilder pattern with HuggingFace tokenizer (if feature enabled)
    #[cfg(feature = "tokenizers")]
    #[test]
    fn test_tokenizer_builder_huggingface() {
        // Try with real tokenizer file first
        let real_tokenizer_path = "models/tinyllama-1.1b-chat-v1.0.Q4_K_M/tokenizer.json";
        
        if !Path::new(real_tokenizer_path).exists() {
            println!("âš ï¸  Real tokenizer file not found, skipping HuggingFace test");
            return;
        }

        let builder = TokenizerBuilder::new(
            Some(TokenizerType::HuggingFace), 
            real_tokenizer_path.to_string()
        );
        
        let tokenizer = match builder.build() {
            Ok(t) => t,
            Err(e) => {
                println!("âš ï¸  Failed to build HuggingFace tokenizer: {}, skipping test", e);
                return;
            }
        };
        
        // Test basic functionality through trait
        assert!(tokenizer.vocab_size() > 0);
        
        // Test encoding
        let text = "hello world";
        let tokens = tokenizer.encode(text).expect("Failed to encode");
        assert!(!tokens.is_empty());
        
        // Test round-trip encoding/decoding
        let decoded = tokenizer.decode(&tokens).expect("Failed to decode");
        assert!(!decoded.is_empty());
    }

    /// Test BPE tokenizer specific functionality
    #[test] 
    fn test_bpe_tokenizer_direct() {
        // Skip if test tokenizer file doesn't exist
        if !Path::new(TEST_TOKENIZER_PATH).exists() {
            println!("âš ï¸  Test tokenizer file not found, skipping BPE direct test");
            return;
        }

        let tokenizer = BPETokenizer::from_tokenizer_json(TEST_TOKENIZER_PATH)
            .expect("Failed to load BPE tokenizer");
        
        // Test vocab size
        assert!(tokenizer.vocab_size() > 100); // Should have our test vocab
        
        // Test specific tokens from our test file
        assert_eq!(tokenizer.token_to_id("hello"), Some(101));
        assert_eq!(tokenizer.token_to_id("world"), Some(102));
        assert_eq!(tokenizer.token_to_id("test"), Some(103));
        
        // Test encoding known words - BPE may not directly encode to single token
        // since it starts with byte-level encoding
        let tokens = tokenizer.encode("hello").expect("Failed to encode hello");
        assert!(!tokens.is_empty()); // Should produce some tokens
        
        // Test that decoding works
        let decoded = tokenizer.decode(&tokens).expect("Failed to decode");
        assert!(!decoded.is_empty());
        
        // Test unknown token handling
        let unknown_tokens = tokenizer.encode("xyz123").expect("Failed to encode unknown");
        assert!(!unknown_tokens.is_empty()); // Should fall back to byte encoding
    }

    /// Test ProductionTokenizer specific functionality (if feature enabled)
    #[cfg(feature = "tokenizers")]
    #[test]
    fn test_production_tokenizer_direct() {
        // Use real tokenizer file
        let real_tokenizer_path = "models/tinyllama-1.1b-chat-v1.0.Q4_K_M/tokenizer.json";
        
        if !Path::new(real_tokenizer_path).exists() {
            println!("âš ï¸  Real tokenizer file not found, skipping Production direct test");
            return;
        }

        let tokenizer = match ProductionTokenizer::from_tokenizer_json(real_tokenizer_path) {
            Ok(t) => t,
            Err(e) => {
                println!("âš ï¸  Failed to load Production tokenizer: {}, skipping test", e);
                return;
            }
        };
        
        // Test vocab size
        assert!(tokenizer.vocab_size() > 0);
        
        // Test encoding and decoding
        let text = "hello world test";
        let tokens = tokenizer.encode(text).expect("Failed to encode");
        let decoded = tokenizer.decode(&tokens).expect("Failed to decode");
        
        // Should be able to round-trip
        assert!(!tokens.is_empty());
        assert!(!decoded.is_empty());
    }

    /// Test TokenizerBuilder with default type
    #[test]
    fn test_tokenizer_builder_default() {
        // Skip if test tokenizer file doesn't exist
        if !Path::new(TEST_TOKENIZER_PATH).exists() {
            println!("âš ï¸  Test tokenizer file not found, skipping default test");
            return;
        }

        let builder = TokenizerBuilder::new(None, TEST_TOKENIZER_PATH.to_string());
        let tokenizer = builder.build().expect("Failed to build default tokenizer");
        
        // Should default to BPE
        assert!(tokenizer.vocab_size() > 0);
        
        let tokens = tokenizer.encode("test").expect("Failed to encode");
        assert!(!tokens.is_empty());
    }

    /// Test error handling for invalid tokenizer path
    #[test]
    fn test_tokenizer_builder_invalid_path() {
        let builder = TokenizerBuilder::new(
            Some(TokenizerType::BPE), 
            "nonexistent/path.json".to_string()
        );
        
        let result = builder.build();
        assert!(result.is_err());
    }

    /// Test BPE tokenizer with basic vocabulary (no file)
    #[test]
    fn test_bpe_tokenizer_basic() {
        let tokenizer = BPETokenizer::new();
        
        // Should have basic byte-level vocab + special tokens
        assert!(tokenizer.vocab_size() >= 260); // 4 special + 256 bytes
        
        // Test special tokens
        assert_eq!(tokenizer.token_to_id("<unk>"), Some(0));
        assert_eq!(tokenizer.token_to_id("<s>"), Some(1));
        assert_eq!(tokenizer.token_to_id("</s>"), Some(2));
        assert_eq!(tokenizer.token_to_id("<pad>"), Some(3));
        
        // Test basic encoding (should work with byte-level)
        let tokens = tokenizer.encode("Hi").expect("Failed to encode");
        assert_eq!(tokens.len(), 2); // 'H' and 'i' as separate bytes
        
        // Test round-trip
        let decoded = tokenizer.decode(&tokens).expect("Failed to decode");
        assert_eq!(decoded, "Hi");
    }

    /// Test both tokenizers with same text and compare results
    #[test]
    fn test_tokenizer_comparison() {
        // Test with basic BPE tokenizer
        let test_texts = ["hello", "world", "Hi"];
        
        // Create BPE tokenizer - always works
        let bpe_tokenizer = BPETokenizer::new();
        
        for text in &test_texts {
            // Test BPE tokenizer
            let bpe_tokens = bpe_tokenizer.encode(text).expect("BPE encode failed");
            let bpe_decoded = bpe_tokenizer.decode(&bpe_tokens).expect("BPE decode failed");
            
            assert!(!bpe_tokens.is_empty(), "BPE tokens should not be empty for '{}'", text);
            // Note: decoded might be empty for some texts due to token filtering
            
            println!("BPE '{}' -> {:?} -> '{}'", text, bpe_tokens, bpe_decoded);
        }
        
        // If we have real tokenizer files, test those too
        #[cfg(feature = "tokenizers")]
        {
            let real_tokenizer_path = "models/tinyllama-1.1b-chat-v1.0.Q4_K_M/tokenizer.json";
            if Path::new(real_tokenizer_path).exists() {
                if let Ok(hf_tokenizer) = TokenizerBuilder::new(
                    Some(TokenizerType::HuggingFace), 
                    real_tokenizer_path.to_string()
                ).build() {
                    for text in &test_texts {
                        let hf_tokens = hf_tokenizer.encode(text).expect("HF encode failed");
                        let hf_decoded = hf_tokenizer.decode(&hf_tokens).expect("HF decode failed");
                        
                        assert!(!hf_tokens.is_empty(), "HF tokens should not be empty for '{}'", text);
                        assert!(!hf_decoded.is_empty(), "HF decoded should not be empty for '{}'", text);
                        
                        println!("HF  '{}' -> {:?} -> '{}'", text, hf_tokens, hf_decoded);
                    }
                }
            }
        }
    }

    /// Test tokenizer trait object usage
    #[test]
    fn test_tokenizer_trait_object() {
        // Create basic BPE tokenizer as trait object
        let tokenizer: Box<dyn Tokenizer> = Box::new(BPETokenizer::new());
        
        // Test that trait methods work
        assert!(tokenizer.vocab_size() > 0);
        
        let text = "Hi";
        let tokens = tokenizer.encode(text).expect("Failed to encode");
        let decoded = tokenizer.decode(&tokens).expect("Failed to decode");
        
        assert!(!tokens.is_empty());
        // Note: decoded might be empty due to token filtering, but that's okay
        
        // Test special token lookup
        assert!(tokenizer.token_to_id("<unk>").is_some());
        assert!(tokenizer.token_to_id("<s>").is_some());
    }

    /// Test TokenizerType enum
    #[test]
    fn test_tokenizer_type_enum() {
        // Test default
        assert_eq!(TokenizerType::default(), TokenizerType::BPE);
        
        // Test equality
        assert_eq!(TokenizerType::BPE, TokenizerType::BPE);
        assert_ne!(TokenizerType::BPE, TokenizerType::HuggingFace);
        
        // Test copy and clone
        let t1 = TokenizerType::HuggingFace;
        let t2 = t1;
        let t3 = t1.clone();
        assert_eq!(t1, t2);
        assert_eq!(t1, t3);
    }

    /// Test that tokenizers can handle empty and edge case inputs
    #[test]
    fn test_tokenizer_edge_cases() {
        let tokenizer = BPETokenizer::new();
        
        // Test empty string
        let empty_tokens = tokenizer.encode("").expect("Failed to encode empty string");
        assert!(empty_tokens.is_empty());
        
        // Test single character
        let single_tokens = tokenizer.encode("a").expect("Failed to encode single char");
        assert_eq!(single_tokens.len(), 1);
        
        // Test decode empty
        let empty_decoded = tokenizer.decode(&[]).expect("Failed to decode empty");
        assert!(empty_decoded.is_empty());
        
        // Test decode unknown token ID - should return [UNK] marker
        let unknown_decoded = tokenizer.decode(&[99999]).expect("Failed to decode unknown");
        assert!(unknown_decoded.contains("[UNK]") || !unknown_decoded.is_empty()); // Should show unknown marker
        
        // Test decode known special token
        let unk_decoded = tokenizer.decode(&[0]).expect("Failed to decode unk token");
        assert!(unk_decoded.contains("[UNK]") || unk_decoded.is_empty()); // UNK token handling
    }

    /// Test batch processing functionality
    #[test]
    fn test_batch_processing() {
        let tokenizer = BPETokenizer::new();
        
        let test_texts = ["Hello", "world", "test batch"];
        
        // Test basic batch encoding
        let batch_results = tokenizer.encode_batch(&test_texts).expect("Batch encoding failed");
        assert_eq!(batch_results.len(), test_texts.len());
        
        for (i, tokens) in batch_results.iter().enumerate() {
            assert!(!tokens.is_empty(), "Tokens should not be empty for text: '{}'", test_texts[i]);
            
            // Verify round-trip works
            let decoded = tokenizer.decode(tokens).expect("Decode failed");
            assert!(!decoded.is_empty(), "Decoded text should not be empty");
        }
        
        // Test batch decoding
        let batch_token_refs: Vec<&[u32]> = batch_results.iter().map(|v| v.as_slice()).collect();
        let decoded_batch = tokenizer.decode_batch(&batch_token_refs).expect("Batch decode failed");
        assert_eq!(decoded_batch.len(), test_texts.len());
        
        for decoded in &decoded_batch {
            assert!(!decoded.is_empty(), "Decoded text should not be empty");
        }
    }

    /// Test batch processing with configuration
    #[test]
    fn test_batch_processing_with_config() {
        let tokenizer = BPETokenizer::new();
        
        let test_texts = ["Hi", "Hello world", "This is a longer text"];
        
        // Test with longest-first padding
        let config = TokenizerConfig {
            max_length: None,
            padding: PaddingStrategy::LongestFirst,
            truncation: false,
            return_attention_mask: true,
            return_offsets: false,
        };
        
        let batch_results = tokenizer.encode_batch_with_config(&test_texts, &config)
            .expect("Batch encoding with config failed");
        
        assert_eq!(batch_results.len(), test_texts.len());
        
        // All sequences should have the same length (padded to longest)
        let lengths: Vec<usize> = batch_results.iter().map(|r| r.input_ids.len()).collect();
        let max_len = lengths.iter().max().unwrap();
        
        for (i, result) in batch_results.iter().enumerate() {
            assert_eq!(result.input_ids.len(), *max_len, "All sequences should be padded to max length");
            assert_eq!(result.attention_mask.len(), *max_len, "Attention mask should match input length");
            assert!(!result.input_ids.is_empty(), "Input IDs should not be empty for text: '{}'", test_texts[i]);
            
            // Check attention mask correctness (1s for real tokens, 0s for padding)
            let original_tokens = tokenizer.encode(test_texts[i]).expect("Encoding failed");
            let original_len = original_tokens.len();
            
            // First original_len tokens should have attention mask = 1
            for j in 0..original_len {
                assert_eq!(result.attention_mask[j], 1, "Real tokens should have attention mask = 1");
            }
            
            // Padding tokens should have attention mask = 0
            for j in original_len..*max_len {
                assert_eq!(result.attention_mask[j], 0, "Padding tokens should have attention mask = 0");
            }
        }
    }

    /// Test enhanced encoding with configuration
    #[test]
    fn test_encode_with_config() {
        let tokenizer = BPETokenizer::new();
        
        let text = "Hello world this is a test";
        
        // Test with truncation
        let config = TokenizerConfig {
            max_length: Some(3),
            padding: PaddingStrategy::None,
            truncation: true,
            return_attention_mask: true,
            return_offsets: true,
        };
        
        let result = tokenizer.encode_with_config(text, &config).expect("Encoding with config failed");
        
        assert_eq!(result.input_ids.len(), 3, "Should be truncated to max_length");
        assert_eq!(result.attention_mask.len(), 3, "Attention mask should match input length");
        assert!(result.offsets.is_some(), "Offsets should be returned when requested");
        
        // Test without truncation (should not truncate)
        let config_no_trunc = TokenizerConfig {
            max_length: Some(3),
            padding: PaddingStrategy::None,
            truncation: false,
            return_attention_mask: true,
            return_offsets: false,
        };
        
        let result_no_trunc = tokenizer.encode_with_config(text, &config_no_trunc)
            .expect("Encoding without truncation failed");
        
        assert!(result_no_trunc.input_ids.len() > 3, "Should not be truncated when truncation=false");
        assert!(result_no_trunc.offsets.is_none(), "Offsets should not be returned when not requested");
    }

    /// Test special token functionality  
    #[test]
    fn test_special_token_functionality() {
        let mut tokenizer = BPETokenizer::new();
        
        // Test getting special tokens
        let special_tokens = tokenizer.get_special_tokens();
        assert!(special_tokens.contains_key("bos_token"));
        assert!(special_tokens.contains_key("eos_token"));
        assert!(special_tokens.contains_key("unk_token"));
        assert!(special_tokens.contains_key("pad_token"));
        
        // Test adding special tokens
        let mut new_tokens = HashMap::new();
        new_tokens.insert("<mask>".to_string(), 999);
        new_tokens.insert("<cls>".to_string(), 1000);
        
        tokenizer.add_special_tokens(&new_tokens).expect("Adding special tokens failed");
        
        assert_eq!(tokenizer.token_to_id("<mask>"), Some(999));
        assert_eq!(tokenizer.token_to_id("<cls>"), Some(1000));
        assert_eq!(tokenizer.get_id_to_token(999), Some("<mask>".to_string()));
        assert_eq!(tokenizer.get_id_to_token(1000), Some("<cls>".to_string()));
    }

    /// Test chat template functionality
    #[test]
    fn test_chat_template() {
        let tokenizer = BPETokenizer::new();
        
        let messages = vec![
            ChatMessage {
                role: "system".to_string(),
                content: "You are a helpful assistant.".to_string(),
            },
            ChatMessage {
                role: "user".to_string(),
                content: "Hello!".to_string(),
            },
            ChatMessage {
                role: "assistant".to_string(),
                content: "Hi there! How can I help you?".to_string(),
            },
        ];
        
        let formatted = tokenizer.apply_chat_template(&messages).expect("Chat template failed");
        
        assert!(!formatted.is_empty(), "Formatted chat should not be empty");
        assert!(formatted.contains("Hello!"), "Should contain user message");
        assert!(formatted.contains("helpful assistant"), "Should contain system message");
        assert!(formatted.contains("Hi there!"), "Should contain assistant message");
        
        println!("Formatted chat: {}", formatted);
    }

    /// Test text processing functions
    #[test]
    fn test_text_processing() {
        let tokenizer = BPETokenizer::new();
        
        // Test normalization
        let text = "  Hello World!  ";
        let normalized = tokenizer.normalize(text);
        assert_eq!(normalized, "Hello World!", "Should trim whitespace");
        
        // Test pre-tokenization
        let text = "Hello world test";
        let pre_tokens = tokenizer.pre_tokenize(text);
        assert_eq!(pre_tokens, vec!["Hello", "world", "test"], "Should split on whitespace");
    }
}
